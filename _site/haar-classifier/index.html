<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no"> <title>Haar Classifier Investigation &#8211; Hand Map</title> <meta name="description" content="My mentor spoke to me about Haar Image Classification with OpenCV. In this post I investigate how it works with a simple object detection example"> <meta name="keywords" content="harr, opencv"> <!-- Twitter Cards --> <meta name="twitter:card" content="summary"> <meta name="twitter:image" content="halve.png"> <meta name="twitter:title" content="Haar Classifier Investigation"> <meta name="twitter:description" content="My mentor spoke to me about Haar Image Classification with OpenCV. In this post I investigate how it works with a simple object detection example"> <meta name="twitter:site" content="@nathangloverAUS"> <meta name="twitter:creator" content="@nathangloverAUS"> <!-- Open Graph --> <meta property="og:locale" content="en_US"> <meta property="og:type" content="article"> <meta property="og:title" content="Haar Classifier Investigation"> <meta property="og:description" content="My mentor spoke to me about Haar Image Classification with OpenCV. In this post I investigate how it works with a simple object detection example"> <meta property="og:url" content="https://handmap.github.io/haar-classifier/"> <meta property="og:site_name" content="Hand Map"> <meta property="og:image" content="https://handmap.github.io/images/halve.png"> <link rel="canonical" href="https://handmap.github.io/haar-classifier/"> <!-- Handheld --> <meta name="HandheldFriendly" content="True"> <meta name="MobileOptimized" content="320"> <meta name="viewport" content="width=device-width, initial-scale=1.0"> <!-- Feed --> <link type="application/atom+xml" rel="alternate" href="https://handmap.github.io/feed.xml" title="Hand Map" /> <!-- Favicons --> <link rel="shortcut icon" type="image/png" href="https://handmap.github.io/favicon.png" /> <link rel="shortcut icon" href="https://handmap.github.io/favicon.ico" /> <!-- CSS --> <link rel="stylesheet" type="text/css" href="https://handmap.github.io/assets/css/main.css"> <!-- Left Block Image for Posts --> <style type="text/css"> #posts.inner-post-page .block-left {background: linear-gradient(rgba(0, 0, 0, 0.5), rgba(0, 0, 0, 0.5)), url(https://handmap.github.io/images/unsplash-gallery-image-3.jpg) no-repeat;background-size: cover;} </style> <!-- Left Block Images for Home and Pages --> <style type="text/css"> #posts .block-left {background: linear-gradient(rgba(0, 0, 0, 0.5), rgba(0, 0, 0, 0.5)), url(https://handmap.github.io/images/unsplash-image-10.jpg) no-repeat;background-size: cover, cover;} .block-left {background: linear-gradient(rgba(44,45,51,0.9), rgba(44,45,51,0.9)), url(https://handmap.github.io/images/home.png) no-repeat;background-size: cover;} </style> </head> <body id="posts" class="inner-post-page"> <div class="block-left"> <div class="content"> <a href="https://handmap.github.io" class="logo"><img src="https://handmap.github.io/images/halve.png"></a> <div class="post-title-section"> <div class="section-line">Posts <em>/</em></div> <h1 class="section-title">Haar Classifier Investigation </h1> <ul class="tags"> <li><a href="https://handmap.github.io/tags#harr">harr</a></li> <li><a href="https://handmap.github.io/tags#opencv">opencv</a></li> </ul> <div class="section-line reverse"><a href="https://handmap.github.io/posts">Back to posts</a> <em>/</em></div> </div> </div> </div> <div class="block-right"> <div class="share-buttons"> <a href="https://twitter.com/intent/tweet?text=https://handmap.github.io/haar-classifier/" class="btn btn_twitter" title="Share on Twitter"><i class="fa fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a> <a href="https://www.facebook.com/sharer/sharer.php?u=https://handmap.github.io/haar-classifier/" class="btn btn_facebook" title="Share on Facebook"><i class="fa fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a> <a href="https://plus.google.com/share?url=https://handmap.github.io/haar-classifier/" class="btn btn_google-plus" title="Share on Google Plus"><i class="fa fa-fw fa-google-plus" aria-hidden="true"></i><span> Google+</span></a> </div> <a href="../posts.html" title="posts" class="posts-menu-icon"></a> <a title="projects" class="projects-menu-icon"> <span></span> </a> <div class="inner-post content"> <div class="date-highlight">11 Aug 2016</div> <h2 id="brief-introduction">Brief Introduction</h2> <hr /> <p>Upon speaking with my mentor about the research topic I was pointed in the direction of Haar Cascade Classification for Object detection. The idea behind this method of detection is to use training data to help detect a particular object in a set of images. The training data itself is typically a few hundred sample views of a particular object; and when we compare these views to a input image we will be given a weighted likelihood on whether or not our image contains the same or similar characteristics are our training data.</p> <p>The following post with be borrowing heavily from the wonderful post by <a href="http://coding-robin.de/2013/07/22/train-your-own-opencv-haar-classifier.html">Thorsten Ball</a>. If I even meet the writer I’ll be sure to buy them a drink.</p> <h2 id="installing-opencv-with-brew">Installing OpenCV with brew</h2> <hr /> <p>I’ve also decided to install OpenCV on my Mac so I’ll quickly include the steps I took to get that done.</p> <h3 id="install-homebrew">Install Homebrew</h3> <hr /> <div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>ruby -e <span class="s2">"</span><span class="k">$(</span>curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install<span class="k">)</span><span class="s2">
</span></code></pre></div> <h3 id="install-opencv-using-brew">Install OpenCV using brew</h3> <hr /> <div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>brew tap homebrew/science
<span class="gp">$ </span>brew install opencv
</code></pre></div> <p>This will more than likely take a long time, so get a cup of coffee (maybe two).</p> <h2 id="getting-started">Getting Started</h2> <hr /> <p>Lets begin by cloning a copy of the repository that the author of the article above has kindly provided. Unfortunately their tutorial was written for OpenCV 2.4.x so whilst we might not be able to execute their code, hopefully we will be able to learn a bit about what is actually required to begin classifying.</p> <div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>git clone https://github.com/mrnugget/opencv-haar-classifier-training
</code></pre></div> <figure> <img src="https://handmap.github.io/images/posts/2016-08-11/Harr-Classifier-Terminal-folders.png" alt="Haar Classifier Folders" /> <figcaption>Folders in cloned repo</figcaption> </figure> <p>You’ll receive a number of folders, each with a different purpose. Lets first focus on <code class="highlighter-rouge">negative_images</code> and <code class="highlighter-rouge">positive_images</code>.</p> <p>In order to train our classifier we first need samples, which means we need a bunch of images that show the object we want to detect (positive sample) and even more images without the object we want (negative sample). The number of images you use will be dependant on what kind of work you are doing;</p> <p>For example, the you’ll need a lot more low quality images to get the same reliability of a system that uses a few very high quality ones. Complex objects are also more likely to need a varying number of images from different angles. It’s also worth noting that the more samples you have the more raw compute power you are likely to need in order to churn our results at an acceptable rate.</p> <p>The tutorial we’re using is just an example, so they opted to use just <code class="highlighter-rouge">40 positive samples</code> and <code class="highlighter-rouge">600 negative samples</code>.</p> <h2 id="positive-samples">Positive Samples</h2> <hr /> <p>Positive samples normally consist of images containing just the object we want to detect. They should be close ups containing as much of the object in the photo as possible; avoid including other objects within the boundaries of the image.</p> <figure> <img src="https://handmap.github.io/images/posts/2016-08-11/opencv_positive_cropped_scaled_01.jpg" alt="Banana Sample 1" /> <img src="https://handmap.github.io/images/posts/2016-08-11/opencv_positive_cropped_scaled_03.jpg" alt="Banana Sample 3" /> <img src="https://handmap.github.io/images/posts/2016-08-11/opencv_positive_cropped_scaled_02.jpg" alt="Banana Sample 2" /> <figcaption>Banana Positive samples by Thorsten Ball</figcaption> </figure> <p>Tips when generating positive samples:</p> <ol> <li>Include as much of the Object you want to detect, and a little of anything else as possible.</li> <li>Get the object from as many different angles as possible.</li> <li>Get the object in as many lighting conditions and with varying backgrounds.</li> </ol> <p>Once you’ve placed the cropped images into the <code class="highlighter-rouge">./positive_images</code> folder run the following command within the root directory of the clones repo:</p> <div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>find ./positive_images -iname <span class="s2">"*.jpg"</span> &gt; positives.txt
</code></pre></div> <p>This command indexes the file names to a list in the positives.txt file in the root directory.</p> <blockquote> <p>NOTE: This command implies that you are using <code class="highlighter-rouge">.jpg</code> files are you source images. You can change the command to suit your needs.</p> </blockquote> <h2 id="negative-samples">Negative Samples</h2> <hr /> <p>Negative images are a little bit more complicated, because they are typically images that don’t show the source Object at all. In fact, the best case scenario is when the negative images are identical to the positives except that don’t contain the Object.</p> <p>The example the post uses is that if we wanted to detect stop signs on walls, the negative image would ideally be a lot of images of walls; or even other signs on walls.</p> <p>The author used 600 negative images in his example, which is quite a few and doesn’t sound too fun. He recommends that if you’re learning you can just extract a video into its frames and use each of those frames as negatives.</p> <p>Tips when generating negative samples:</p> <ol> <li>Image doesn’t include the object in them at all</li> <li>Image contains similar backgrounds or environments just without the object present</li> <li>Use HEAPS of them.</li> </ol> <p>Now lets run the equivalent command for negatives to generate our negatives.txt</p> <div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>find ./negative_images -iname <span class="s2">"*.jpg"</span> &gt; negatives.txt
</code></pre></div> <h2 id="creating-samples">Creating Samples</h2> <hr /> <p>The next step is to generate samples out of the positive/negative images we just imported. We can use a utility that comes with OpenCV called createsamples that will enumerate over our sample images and generate a large number of positive samples from our positive images, by applying transformations and distortions to them.</p> <p><a href="http://note.sonots.com/SciSoftware/haartraining.html#w0a08ab4">Naotoshi Seo</a> provided a very helpful perl script that we’ll be using found in <code class="highlighter-rouge">/bin/createsamples.pl</code> of the cloned repo. We’ll use it with a couple arguments to generate roughly 1500 positive samples, by compiling each positive image with a random negative image and then running them through the official <a href="http://docs.opencv.org/2.4/doc/user_guide/ug_traincascade.html#positive-samples">opencv_createsamples</a> library.</p> <div class="language-perl highlighter-rouge"><pre class="highlight"><code><span class="nv">$</span> <span class="nv">perl</span> <span class="nv">bin</span><span class="o">/</span><span class="nv">createsamples</span><span class="o">.</span><span class="nv">pl</span> <span class="nv">positives</span><span class="o">.</span><span class="nv">txt</span> <span class="nv">negatives</span><span class="o">.</span><span class="nv">txt</span> <span class="nv">samples</span> <span class="mi">1500</span><span class="o">\</span>
    <span class="s">"opencv_createsamples -bgcolor 0 -bgthresh 0 -maxxangle 1.1\
    -maxyangle 1.1 maxzangle 0.5 -maxidev 40 -w 80 -h 40"</span>
</code></pre></div> <blockquote> <p>Pay close attention to the <code class="highlighter-rouge">-w</code> and <code class="highlighter-rouge">-h</code> arguements in the above script. The values for these should closely match the image ratio of your positive images.</p> </blockquote> <p>The last task here is to merge the <code class="highlighter-rouge">*.vec</code> file that were output from the previous command that were placed in the <code class="highlighter-rouge">samples</code> directory. There’s another useful tool by Blake Wulfe in the tools folder of the repo source called <code class="highlighter-rouge">mergevec,py</code>. We’ll be using this to merge out samples.</p> <p>Lets start by first compiling a list of the <code class="highlighter-rouge">*.vec</code> files into samples.txt</p> <div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>find ./samples -name <span class="s1">'*.vec'</span> &gt; samples.txt
</code></pre></div> <p>and now lets execute the python script with the required arguments.</p> <div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>python /tools/mergevec.py -v samples.txt -o samples.vec
</code></pre></div> <p>We now have a <code class="highlighter-rouge">samples.vec</code> file that we can use to start training out classifier.</p> <h2 id="training-the-classifier">Training the Classifier</h2> <hr /> <p>Finally we’ll use the <code class="highlighter-rouge">opencv_traincascade</code> library to generate our classifiers. This can be done with the following lines in your command line.</p> <div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>opencv_traincascade -data classifier -vec samples.vec -bg negatives.txt<span class="se">\</span>
    -numStages 20 -minHitRate 0.999 -maxFalseAlarmRate 0.5 -numPos 1000<span class="se">\</span>
    -numNeg 600 -w 80 -h 40 -mode ALL -precalcValBufSize 1024<span class="se">\</span>
    -precalcIdxBufSize 1024
</code></pre></div> <p>The important arguments are <code class="highlighter-rouge">-numNeg</code> that specifies the number of negative samples we have, the <code class="highlighter-rouge">-precalcValBufSize</code> and <code class="highlighter-rouge">-precalcIdxBufSize</code> define how much memory to use while training and <code class="highlighter-rouge">-numPos</code> should be lower than the positive samples we generated.</p> <blockquote> <p>WARNING: this will take a VERY long time. the author advised that this isn’t a case of “get a cup of coffee and have a shower”. When he ran it, it took a couple days on a decent Macbook from 2011</p> </blockquote> <p>It’s also worth noting that you don’t have to keep the process running without any interruptions; you can stop and restart it any time and it will continue where it left off.</p> <p>Once the process completes, you’ll have a file called <code class="highlighter-rouge">classifier.xml</code> in the <code class="highlighter-rouge">classifier</code> directory. This is the classifier that defines our object detection.</p> <h2 id="nodejs-and-opencv">NodeJS and OpenCV</h2> <hr /> <p>Now that we have the classifications it’s fairly straightforward to run it on some sample images. The tutorial I worked through used the <a href="https://github.com/peterbraden/node-opencv">node-opencv</a> module that can be installed using <a href="https://www.npmjs.com/">npm</a></p> <h3 id="install-nodejs-and-npm">Install NodeJS and NPM</h3> <hr /> <div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>brew install node
</code></pre></div> <h3 id="install-node-opencv">Install node-opencv</h3> <hr /> <div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>npm install opencv
</code></pre></div> <h3 id="recognizethisjs">recognize_this.js</h3> <hr /> <p>We’ll be using an example javascript file that takes a number of input files and spits out processed versions of the files. The code taken from <a href="https://github.com/peterbraden/node-opencv/blob/master/examples/Face.js">this repo</a> can be seen below:</p> <div class="language-javascript highlighter-rouge"><pre class="highlight"><code><span class="kd">var</span> <span class="nx">cv</span> <span class="o">=</span> <span class="nx">require</span><span class="p">(</span><span class="s1">'opencv'</span><span class="p">);</span>

<span class="kd">var</span> <span class="nx">color</span>       <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="mi">0</span><span class="p">];</span>
<span class="kd">var</span> <span class="nx">thickness</span>   <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
<span class="kd">var</span> <span class="nx">cascadeFile</span> <span class="o">=</span> <span class="s1">'./my_cascade.xml'</span><span class="p">;</span>

<span class="kd">var</span> <span class="nx">inputFiles</span> <span class="o">=</span> <span class="p">[</span>
  <span class="s1">'./recognize_this_1.jpg'</span><span class="p">,</span> <span class="s1">'./recognize_this_2.jpg'</span><span class="p">,</span> <span class="s1">'./recognize_this_3.jpg'</span><span class="p">,</span>
  <span class="s1">'./recognize_this_3.jpg'</span><span class="p">,</span> <span class="s1">'./recognize_this_4.jpg'</span><span class="p">,</span> <span class="s1">'./recognize_this_5.jpg'</span>
<span class="p">];</span>

<span class="nx">inputFiles</span><span class="p">.</span><span class="nx">forEach</span><span class="p">(</span><span class="kd">function</span><span class="p">(</span><span class="nx">fileName</span><span class="p">)</span> <span class="p">{</span>
  <span class="nx">cv</span><span class="p">.</span><span class="nx">readImage</span><span class="p">(</span><span class="nx">fileName</span><span class="p">,</span> <span class="kd">function</span><span class="p">(</span><span class="nx">err</span><span class="p">,</span> <span class="nx">im</span><span class="p">)</span> <span class="p">{</span>
    <span class="nx">im</span><span class="p">.</span><span class="nx">detectObject</span><span class="p">(</span><span class="nx">cascadeFile</span><span class="p">,</span> <span class="p">{</span><span class="na">neighbors</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="na">scale</span><span class="p">:</span> <span class="mi">2</span><span class="p">},</span> <span class="kd">function</span><span class="p">(</span><span class="nx">err</span><span class="p">,</span> <span class="nx">objects</span><span class="p">)</span> <span class="p">{</span>
      <span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="nx">objects</span><span class="p">);</span>
      <span class="k">for</span><span class="p">(</span><span class="kd">var</span> <span class="nx">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="nx">k</span> <span class="o">&lt;</span> <span class="nx">objects</span><span class="p">.</span><span class="nx">length</span><span class="p">;</span> <span class="nx">k</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="kd">var</span> <span class="nx">object</span> <span class="o">=</span> <span class="nx">objects</span><span class="p">[</span><span class="nx">k</span><span class="p">];</span>
        <span class="nx">im</span><span class="p">.</span><span class="nx">rectangle</span><span class="p">(</span>
          <span class="p">[</span><span class="nx">object</span><span class="p">.</span><span class="nx">x</span><span class="p">,</span> <span class="nx">object</span><span class="p">.</span><span class="nx">y</span><span class="p">],</span>
          <span class="p">[</span><span class="nx">object</span><span class="p">.</span><span class="nx">x</span> <span class="o">+</span> <span class="nx">object</span><span class="p">.</span><span class="nx">width</span><span class="p">,</span> <span class="nx">object</span><span class="p">.</span><span class="nx">y</span> <span class="o">+</span> <span class="nx">object</span><span class="p">.</span><span class="nx">height</span><span class="p">],</span>
          <span class="nx">color</span><span class="p">,</span>
          <span class="mi">2</span>
        <span class="p">);</span>
      <span class="p">}</span>
      <span class="nx">im</span><span class="p">.</span><span class="nx">save</span><span class="p">(</span><span class="nx">fileName</span><span class="p">.</span><span class="nx">replace</span><span class="p">(</span><span class="sr">/</span><span class="se">\.</span><span class="sr">jpg/</span><span class="p">,</span> <span class="s1">'processed.jpg'</span><span class="p">));</span>
    <span class="p">});</span>
  <span class="p">});</span>
<span class="p">});</span>
</code></pre></div> <blockquote> <p>Note that the variable delaring <code class="highlighter-rouge">my_cascade.xml</code> should match the trained classification file we generated prior.</p> </blockquote> <p>Alternatively if you want to use the python environment we setup in previous tutorials simply use the following code:</p> <div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cv2</span>

<span class="n">banana_cascade</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">CascadeClassifier</span><span class="p">(</span><span class="s">'banana_classifier.xml'</span><span class="p">)</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s">'bananas-main.jpg'</span><span class="p">)</span>
<span class="n">gray</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2GRAY</span><span class="p">)</span>

<span class="n">bananas</span> <span class="o">=</span> <span class="n">banana_cascade</span><span class="o">.</span><span class="n">detectMultiScale</span><span class="p">(</span><span class="n">gray</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="k">for</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span> <span class="p">,</span><span class="n">h</span><span class="p">)</span> <span class="ow">in</span> <span class="n">bananas</span><span class="p">:</span>
    <span class="n">cv2</span><span class="o">.</span><span class="n">rectangle</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="p">(</span><span class="n">x</span><span class="o">+</span><span class="n">w</span><span class="p">,</span> <span class="n">y</span><span class="o">+</span><span class="n">h</span><span class="p">),(</span><span class="mi">255</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span><span class="mi">2</span><span class="p">)</span>

<span class="n">cv2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s">'img'</span><span class="p">,</span> <span class="n">img</span><span class="p">)</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">cv2</span><span class="o">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>
</code></pre></div> <p>Below is an example of a working output image. It is able to detect that there are three bananas in the image and places a box around each of them.</p> <figure> <img src="https://handmap.github.io/images/posts/2016-08-11/openCV-banana-detect-pass.png" alt="Banana Detect Pass" /> <figcaption>Banana Detect Pass</figcaption> </figure> <p>It is however very obvious that our training set wasn’t deep enough when we get outputs like the one below:</p> <figure> <img src="https://handmap.github.io/images/posts/2016-08-11/openCV-banana-detect-failed.png" alt="Banana Detect Pass" /> <figcaption>Banana Detect Failed</figcaption> </figure> <p>The failure is most likely because the training set didn’t have enough variations of the Banana that included it in that orientation. If we look at the successful detections we can also note that the classification seems to pick up the very yellow parts of the banana; something that the image with the man didn’t have.</p> <h2 id="conclusion">Conclusion</h2> <hr /> <p>I’d like to conclude on the note that the Haar Classification process is very time consuming and requires a lot of trial and error with data sets. It also doesn’t help that the set take such a long time to generate. If I do decide to go down the path of using Haar, I might need to make use of some existing Hand classification data sets instead of making my own.</p> <br> <a href="https://twitter.com/intent/tweet?text=https://handmap.github.io/haar-classifier/" class="btn btn_twitter" title="Share on Twitter"><i class="fa fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a> <a href="https://www.facebook.com/sharer/sharer.php?u=https://handmap.github.io/haar-classifier/" class="btn btn_facebook" title="Share on Facebook"><i class="fa fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a> <a href="https://plus.google.com/share?url=https://handmap.github.io/haar-classifier/" class="btn btn_google-plus" title="Share on Google Plus"><i class="fa fa-fw fa-google-plus" aria-hidden="true"></i><span> Google+</span></a> <nav class="pagination"> <a href="https://handmap.github.io/exploring-opencv-with-python/" class="pagination_pager" title="Exploring OpenCV with Python ">previous</a> <a href="#" class="pagination_pager disabled">next</a> </nav> </div> </div> <!-- JS --> <script src="https://handmap.github.io/assets/js/main.min.js"></script> <div class="overlay"> <ul class="projects-menu"> <li style="background:url(https://github.com/HandMap/HandMap.github.io/blob/master/images/project.jpg) center center no-repeat;"> <a href="https://handmap.github.io" class="inactive" target="_blank" rel="nofollow external"> <span> Hand Map Project <br><em>in progress</em> </span> </a> </li> </ul> </div> </body> </html>
