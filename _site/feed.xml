<?xml version="1.0" encoding="utf-8"?><?xml-stylesheet type="text/xml" href="/feed.xslt.xml"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.2.1">Jekyll</generator><link href="https://handmap.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://handmap.github.io/" rel="alternate" type="text/html" /><updated>2016-08-15T17:46:44+08:00</updated><id>https://handmap.github.io/</id><title>Hand Map</title><subtitle>The Official Hand Map Project Blog</subtitle><entry><title>Haar Classifier Investigation</title><link href="https://handmap.github.io/haar-classifier/" rel="alternate" type="text/html" title="Haar Classifier Investigation" /><published>2016-08-11T00:00:00+08:00</published><updated>2016-08-11T00:00:00+08:00</updated><id>https://handmap.github.io/haar-classifier</id><content type="html" xml:base="https://handmap.github.io/haar-classifier/">&lt;h2 id=&quot;brief-introduction&quot;&gt;Brief Introduction&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Upon speaking with my mentor about the research topic I was pointed in the direction of Haar Cascade Classification for Object detection. The idea behind this method of detection is to use training data to help detect a particular object in a set of images. The training data itself is typically a few hundred sample views of a particular object; and when we compare these views to a input image we will be given a weighted likelihood on whether or not our image contains the same or similar characteristics are our training data.&lt;/p&gt;

&lt;p&gt;The following post with be borrowing heavily from the wonderful post by &lt;a href=&quot;http://coding-robin.de/2013/07/22/train-your-own-opencv-haar-classifier.html&quot;&gt;Thorsten Ball&lt;/a&gt;. If I even meet the writer I’ll be sure to buy them a drink.&lt;/p&gt;

&lt;h2 id=&quot;installing-opencv-with-brew&quot;&gt;Installing OpenCV with brew&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;I’ve also decided to install OpenCV on my Mac so I’ll quickly include the steps I took to get that done.&lt;/p&gt;

&lt;h3 id=&quot;install-homebrew&quot;&gt;Install Homebrew&lt;/h3&gt;
&lt;hr /&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ruby -e &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;$(&lt;/span&gt;curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install&lt;span class=&quot;k&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;install-opencv-using-brew&quot;&gt;Install OpenCV using brew&lt;/h3&gt;
&lt;hr /&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;brew tap homebrew/science
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;brew install opencv
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This will more than likely take a long time, so get a cup of coffee (maybe two).&lt;/p&gt;

&lt;h2 id=&quot;getting-started&quot;&gt;Getting Started&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Lets begin by cloning a copy of the repository that the author of the article above has kindly provided. Unfortunately their tutorial was written for OpenCV 2.4.x so whilst we might not be able to execute their code, hopefully we will be able to learn a bit about what is actually required to begin classifying.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;git clone https://github.com/mrnugget/opencv-haar-classifier-training
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;figure&gt;
  
&lt;img src=&quot;https://handmap.github.io/images/posts/2016-08-11/Harr-Classifier-Terminal-folders.png&quot; alt=&quot;Haar Classifier Folders&quot; /&gt;

  &lt;figcaption&gt;Folders in cloned repo&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;You’ll receive a number of folders, each with a different purpose. Lets first focus on &lt;code class=&quot;highlighter-rouge&quot;&gt;negative_images&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;positive_images&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In order to train our classifier we first need samples, which means we need a bunch of images that show the object we want to detect (positive sample) and even more images without the object we want (negative sample). The number of images you use will be dependant on what kind of work you are doing;&lt;/p&gt;

&lt;p&gt;For example, the you’ll need a lot more low quality images to get the same reliability of a system that uses a few very high quality ones. Complex objects are also more likely to need a varying number of images from different angles. It’s also worth noting that the more samples you have the more raw compute power you are likely to need in order to churn our results at an acceptable rate.&lt;/p&gt;

&lt;p&gt;The tutorial we’re using is just an example, so they opted to use just &lt;code class=&quot;highlighter-rouge&quot;&gt;40 positive samples&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;600 negative samples&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;positive-samples&quot;&gt;Positive Samples&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Positive samples normally consist of images containing just the object we want to detect. They should be close ups containing as much of the object in the photo as possible; avoid including other objects within the boundaries of the image.&lt;/p&gt;

&lt;figure&gt;
  
&lt;img src=&quot;https://handmap.github.io/images/posts/2016-08-11/opencv_positive_cropped_scaled_01.jpg&quot; alt=&quot;Banana Sample 1&quot; /&gt;
&lt;img src=&quot;https://handmap.github.io/images/posts/2016-08-11/opencv_positive_cropped_scaled_03.jpg&quot; alt=&quot;Banana Sample 3&quot; /&gt;
&lt;img src=&quot;https://handmap.github.io/images/posts/2016-08-11/opencv_positive_cropped_scaled_02.jpg&quot; alt=&quot;Banana Sample 2&quot; /&gt;

  &lt;figcaption&gt;Banana Positive samples by Thorsten Ball&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Tips when generating positive samples:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Include as much of the Object you want to detect, and a little of anything else as possible.&lt;/li&gt;
  &lt;li&gt;Get the object from as many different angles as possible.&lt;/li&gt;
  &lt;li&gt;Get the object in as many lighting conditions and with varying backgrounds.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Once you’ve placed the cropped images into the &lt;code class=&quot;highlighter-rouge&quot;&gt;./positive_images&lt;/code&gt; folder run the following command within the root directory of the clones repo:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;find ./positive_images -iname &lt;span class=&quot;s2&quot;&gt;&quot;*.jpg&quot;&lt;/span&gt; &amp;gt; positives.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This command indexes the file names to a list in the positives.txt file in the root directory.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;NOTE: This command implies that you are using &lt;code class=&quot;highlighter-rouge&quot;&gt;.jpg&lt;/code&gt; files are you source images. You can change the command to suit your needs.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;negative-samples&quot;&gt;Negative Samples&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Negative images are a little bit more complicated, because they are typically images that don’t show the source Object at all. In fact, the best case scenario is when the negative images are identical to the positives except that don’t contain the Object.&lt;/p&gt;

&lt;p&gt;The example the post uses is that if we wanted to detect stop signs on walls, the negative image would ideally be a lot of images of walls; or even other signs on walls.&lt;/p&gt;

&lt;p&gt;The author used 600 negative images in his example, which is quite a few and doesn’t sound too fun. He recommends that if you’re learning you can just extract a video into its frames and use each of those frames as negatives.&lt;/p&gt;

&lt;p&gt;Tips when generating negative samples:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Image doesn’t include the object in them at all&lt;/li&gt;
  &lt;li&gt;Image contains similar backgrounds or environments just without the object present&lt;/li&gt;
  &lt;li&gt;Use HEAPS of them.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now lets run the equivalent command for negatives to generate our negatives.txt&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;find ./negative_images -iname &lt;span class=&quot;s2&quot;&gt;&quot;*.jpg&quot;&lt;/span&gt; &amp;gt; negatives.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;creating-samples&quot;&gt;Creating Samples&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;The next step is to generate samples out of the positive/negative images we just imported. We can use a utility that comes with OpenCV called createsamples that will enumerate over our sample images and generate a large number of positive samples from our positive images, by applying transformations and distortions to them.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://note.sonots.com/SciSoftware/haartraining.html#w0a08ab4&quot;&gt;Naotoshi Seo&lt;/a&gt; provided a very helpful perl script that we’ll be using found in &lt;code class=&quot;highlighter-rouge&quot;&gt;/bin/createsamples.pl&lt;/code&gt; of the cloned repo. We’ll use it with a couple arguments to generate roughly 1500 positive samples, by compiling each positive image with a random negative image and then running them through the official &lt;a href=&quot;http://docs.opencv.org/2.4/doc/user_guide/ug_traincascade.html#positive-samples&quot;&gt;opencv_createsamples&lt;/a&gt; library.&lt;/p&gt;

&lt;div class=&quot;language-perl highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;perl&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;bin&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;createsamples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;pl&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;positives&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;txt&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;negatives&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;txt&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;samples&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1500&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;opencv_createsamples -bgcolor 0 -bgthresh 0 -maxxangle 1.1\
    -maxyangle 1.1 maxzangle 0.5 -maxidev 40 -w 80 -h 40&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;Pay close attention to the &lt;code class=&quot;highlighter-rouge&quot;&gt;-w&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;-h&lt;/code&gt; arguements in the above script. The values for these should closely match the image ratio of your positive images.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The last task here is to merge the &lt;code class=&quot;highlighter-rouge&quot;&gt;*.vec&lt;/code&gt; file that were output from the previous command that were placed in the &lt;code class=&quot;highlighter-rouge&quot;&gt;samples&lt;/code&gt; directory. There’s another useful tool by Blake Wulfe in the tools folder of the repo source called &lt;code class=&quot;highlighter-rouge&quot;&gt;mergevec,py&lt;/code&gt;. We’ll be using this to merge out samples.&lt;/p&gt;

&lt;p&gt;Lets start by first compiling a list of the &lt;code class=&quot;highlighter-rouge&quot;&gt;*.vec&lt;/code&gt; files into samples.txt&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;find ./samples -name &lt;span class=&quot;s1&quot;&gt;'*.vec'&lt;/span&gt; &amp;gt; samples.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;and now lets execute the python script with the required arguments.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;python /tools/mergevec.py -v samples.txt -o samples.vec
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;We now have a &lt;code class=&quot;highlighter-rouge&quot;&gt;samples.vec&lt;/code&gt; file that we can use to start training out classifier.&lt;/p&gt;

&lt;h2 id=&quot;training-the-classifier&quot;&gt;Training the Classifier&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Finally we’ll use the &lt;code class=&quot;highlighter-rouge&quot;&gt;opencv_traincascade&lt;/code&gt; library to generate our classifiers. This can be done with the following lines in your command line.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;opencv_traincascade -data classifier -vec samples.vec -bg negatives.txt&lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    -numStages 20 -minHitRate 0.999 -maxFalseAlarmRate 0.5 -numPos 1000&lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    -numNeg 600 -w 80 -h 40 -mode ALL -precalcValBufSize 1024&lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    -precalcIdxBufSize 1024
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The important arguments are &lt;code class=&quot;highlighter-rouge&quot;&gt;-numNeg&lt;/code&gt; that specifies the number of negative samples we have, the &lt;code class=&quot;highlighter-rouge&quot;&gt;-precalcValBufSize&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;-precalcIdxBufSize&lt;/code&gt; define how much memory to use while training and &lt;code class=&quot;highlighter-rouge&quot;&gt;-numPos&lt;/code&gt; should be lower than the positive samples we generated.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;WARNING: this will take a VERY long time. the author advised that this isn’t a case of “get a cup of coffee and have a shower”. When he ran it, it took a couple days on a decent Macbook from 2011&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;It’s also worth noting that you don’t have to keep the process running without any interruptions; you can stop and restart it any time and it will continue where it left off.&lt;/p&gt;

&lt;p&gt;Once the process completes, you’ll have a file called &lt;code class=&quot;highlighter-rouge&quot;&gt;classifier.xml&lt;/code&gt; in the &lt;code class=&quot;highlighter-rouge&quot;&gt;classifier&lt;/code&gt; directory. This is the classifier that defines our object detection.&lt;/p&gt;

&lt;h2 id=&quot;nodejs-and-opencv&quot;&gt;NodeJS and OpenCV&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Now that we have the classifications it’s fairly straightforward to run it on some sample images. The tutorial I worked through used the &lt;a href=&quot;https://github.com/peterbraden/node-opencv&quot;&gt;node-opencv&lt;/a&gt; module that can be installed using &lt;a href=&quot;https://www.npmjs.com/&quot;&gt;npm&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;install-nodejs-and-npm&quot;&gt;Install NodeJS and NPM&lt;/h3&gt;
&lt;hr /&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;brew install node
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;install-node-opencv&quot;&gt;Install node-opencv&lt;/h3&gt;
&lt;hr /&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;npm install opencv
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;recognizethisjs&quot;&gt;recognize_this.js&lt;/h3&gt;
&lt;hr /&gt;

&lt;p&gt;We’ll be using an example javascript file that takes a number of input files and spits out processed versions of the files. The code taken from &lt;a href=&quot;https://github.com/peterbraden/node-opencv/blob/master/examples/Face.js&quot;&gt;this repo&lt;/a&gt; can be seen below:&lt;/p&gt;

&lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;cv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;require&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'opencv'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;color&lt;/span&gt;       &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
&lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;thickness&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;cascadeFile&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'./my_cascade.xml'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;inputFiles&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
  &lt;span class=&quot;s1&quot;&gt;'./recognize_this_1.jpg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'./recognize_this_2.jpg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'./recognize_this_3.jpg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;s1&quot;&gt;'./recognize_this_3.jpg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'./recognize_this_4.jpg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'./recognize_this_5.jpg'&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;

&lt;span class=&quot;nx&quot;&gt;inputFiles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;forEach&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;fileName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;readImage&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;fileName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;err&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;im&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;im&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;detectObject&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;cascadeFile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;neighbors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;err&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;objects&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;objects&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;objects&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;object&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;objects&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
        &lt;span class=&quot;nx&quot;&gt;im&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;rectangle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;width&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;height&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
          &lt;span class=&quot;nx&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
      &lt;span class=&quot;nx&quot;&gt;im&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;fileName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sr&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\.&lt;/span&gt;&lt;span class=&quot;sr&quot;&gt;jpg/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'processed.jpg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;Note that the variable delaring &lt;code class=&quot;highlighter-rouge&quot;&gt;my_cascade.xml&lt;/code&gt; should match the trained classification file we generated prior.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Alternatively if you want to use the python environment we setup in previous tutorials simply use the following code:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;cv2&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;banana_cascade&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CascadeClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'banana_classifier.xml'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'bananas-main.jpg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;gray&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cvtColor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;COLOR_BGR2GRAY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;bananas&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;banana_cascade&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detectMultiScale&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bananas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rectangle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'img'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;waitKey&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;destroyAllWindows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Below is an example of a working output image. It is able to detect that there are three bananas in the image and places a box around each of them.&lt;/p&gt;

&lt;figure&gt;
  
&lt;img src=&quot;https://handmap.github.io/images/posts/2016-08-11/openCV-banana-detect-pass.png&quot; alt=&quot;Banana Detect Pass&quot; /&gt;

  &lt;figcaption&gt;Banana Detect Pass&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;It is however very obvious that our training set wasn’t deep enough when we get outputs like the one below:&lt;/p&gt;

&lt;figure&gt;
  
&lt;img src=&quot;https://handmap.github.io/images/posts/2016-08-11/openCV-banana-detect-failed.png&quot; alt=&quot;Banana Detect Pass&quot; /&gt;

  &lt;figcaption&gt;Banana Detect Failed&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The failure is most likely because the training set didn’t have enough variations of the Banana in that orientation. It’s interesting to see that it picked up the curvature of the mans smile as a banana because most of our training data would have provided a very similar shape to that of his chin.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;I’d like to conclude on the note that the Haar Classification process is very time consuming and requires a lot of trial and error with data sets. It also doesn’t help that the set take such a long time to generate. If I do decide to go down the path of using Haar, I might need to make use of some existing Hand classification data sets instead of making my own.&lt;/p&gt;</content><category term="harr" /><category term="opencv" /><summary>My mentor spoke to me about Haar Image Classification with OpenCV. In this post I investigate how it works with a simple object detection example</summary></entry><entry><title>Exploring OpenCV with Python</title><link href="https://handmap.github.io/exploring-opencv-with-python/" rel="alternate" type="text/html" title="Exploring OpenCV with Python" /><published>2016-08-06T00:00:00+08:00</published><updated>2016-08-06T00:00:00+08:00</updated><id>https://handmap.github.io/exploring-opencv-with-python</id><content type="html" xml:base="https://handmap.github.io/exploring-opencv-with-python/">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Today I begin my swim into the murky waters that is &lt;a href=&quot;http://opencv.org/&quot;&gt;OpenCV&lt;/a&gt; (Open Computer Vision). To help accommodate my learning I’ve decided to use the excellent resource by Adrian Rosebrock titled &lt;a href=&quot;https://www.pyimagesearch.com/practical-python-opencv/&quot;&gt;Practical Python and OpenCV&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The main goals of this session are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Build a better understanding of what the OpenCV libraries are actually capable of rather than continuing to copy code from random examples expecting everything to just work.&lt;/li&gt;
  &lt;li&gt;Build a base class to work with that includes an easy method of drawing video from an interfaced camera onto a display.&lt;/li&gt;
  &lt;li&gt;Perform some kind of analysis on the captured video/images.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;python-opencv-requirement-overview&quot;&gt;Python OpenCV Requirement overview&lt;/h2&gt;

&lt;h3 id=&quot;opencv&quot;&gt;opencv&lt;/h3&gt;
&lt;hr /&gt;
&lt;p&gt;OpenCV itself can be rather painful to install on Linux and OSX. &lt;a href=&quot;http://www.pyimagesearch.com/opencv-tutorials-resources-guides/&quot;&gt;THIS&lt;/a&gt; is a fairly useful resource that should be kept fairly up to date when versions change. OpenCV is written in C/C++ at its core; however there are plenty of other lanaguages that have official binding into the package.&lt;/p&gt;

&lt;h3 id=&quot;numpy&quot;&gt;numpy&lt;/h3&gt;
&lt;hr /&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;pip install numpy 
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;a href=&quot;http://www.numpy.org/&quot;&gt;NumPy&lt;/a&gt; is a python library that provides support for large multidimensional arrays. This is important to us because in order to analyse images we need a data structure capable of representing images as data. Take a look at an example of this below that is taken from the &lt;a href=&quot;https://www.tensorflow.org/versions/r0.9/tutorials/mnist/beginners/index.html&quot;&gt;TensorFlow MNIST For ML Beginners tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;figure&gt;
  
&lt;img src=&quot;https://handmap.github.io/images/posts/2016-08-06/MNIST-Matrix.png&quot; alt=&quot;TensorFlow MNIST&quot; /&gt;

  &lt;figcaption&gt;TensorFlow MNIST image to multidimensional array&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;As you can see the pixels associated with the shape are given weighted values indicating how dark/different they are from the background. This is in a sense how pixels on a display work, except they use RGB (0-&amp;gt;255) colour values and other methods to represent their data.&lt;/p&gt;

&lt;h3 id=&quot;scipy&quot;&gt;scipy&lt;/h3&gt;
&lt;hr /&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;pip install scipy 
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;a href=&quot;https://www.scipy.org/&quot;&gt;SciPy&lt;/a&gt; is a package that goes hand-in-hand with NumPy. It provides extended support for highly optimized and efficient scientific computing.&lt;/p&gt;

&lt;h3 id=&quot;matplotlib&quot;&gt;matplotlib&lt;/h3&gt;
&lt;hr /&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;pip install matplotlib 
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;a href=&quot;http://matplotlib.org/&quot;&gt;matplotlib&lt;/a&gt; is a plotting library similar to MATLAB. We will be using it extensively to analyse and plot the image data we capture.&lt;/p&gt;

&lt;h3 id=&quot;mahotas&quot;&gt;mahotas&lt;/h3&gt;
&lt;hr /&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;pip install mahotas 
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;a href=&quot;http://mahotas.readthedocs.io/en/latest/&quot;&gt;Mahotas&lt;/a&gt; is a computer vision library written explicitely for the python language. Most of its functionality can already be found inside OpenCV however there are some edge cases where Mahotas will need to be used due to limitations on the OpenCV libary.&lt;/p&gt;

&lt;h3 id=&quot;scikit-learn&quot;&gt;scikit-learn&lt;/h3&gt;
&lt;hr /&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;pip install scikit-learn 
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;a href=&quot;http://scikit-learn.org/&quot;&gt;scikit-learn&lt;/a&gt; is the machine learning library of choice when we deal with OpenCV. Early on it won’t be used often as our datasets will be quite limited, however overtime we might find a usecase for deep data analysis.&lt;/p&gt;

&lt;h3 id=&quot;scikit-image&quot;&gt;scikit-image&lt;/h3&gt;
&lt;hr /&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;pip install -U scikit-image 
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;a href=&quot;http://scikit-image.org/&quot;&gt;scikit-image&lt;/a&gt; provides some fantastic computer vision libraries that are maintained and optimised tremendously well.&lt;/p&gt;

&lt;h2 id=&quot;loading-displaying-and-saving-images&quot;&gt;Loading, Displaying and Saving images&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Lets get started with setting up a simple program to load an image and display it on the screen. To do we’ll first need to import all the libraries we’re going to need. This can be done using the following code:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# The top import is for Python2.7 support&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;__future__&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;print_function&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;argparse&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;cv2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Next we setup some code to handle the command line arguements to take in the path to an image we wish to load from. We use the &lt;a href=&quot;https://docs.python.org/3/library/argparse.html&quot;&gt;argparse&lt;/a&gt; library for this.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# argparse to handle our command line arguments&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ap&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;argparse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ArgumentParser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_argument&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;-i&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;--image&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;required&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;help&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Path to the image&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# parse the arguments and store them in a dictionary&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;vars&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parse_args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Now we can write up some code to read in the image, process it and display it on the screen.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# load the image off the disk&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# the imread function returns a NumPy array representing the image.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;image&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# examine the dimensions of the image. The images are represented&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# as NumPy arrays, so we can use the shape attribute to examine&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# the width, height, and number of channels&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;width: {} pixels&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;height: {} pixels&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;channels: {}&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# handles displaying the actual image on my screen&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# the first parameter is a string, the &quot;title&quot; of the window&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# the second parameter is a reference to the image we loaded.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Image&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# the cv2.waitKey paused the execution of the script until&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# we press a key on the keyboard.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;waitKey&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# lastly we write our image to file in JPG format&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# new_image.jpg is just the path to the new file&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imwrite&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;new_image.jpg&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Now to test it’s as simple as following the command line arguments we setup. I’ve referenced an image ../images/trex.png in this example. This file was located in a folder called ‘images’ one step up in the folder hierarchy.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;python load_display_save.py --image ../images/trex.png
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Running this presents the following information in the console:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;﻿width: 350 pixels
height: 228 pixels
channels: 3
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This data is the basic information that makes up our image. The height and width are expressed in pixels and you’ll also see 3 channel mentioned which represent the RGB components of the image file.&lt;/p&gt;

&lt;p&gt;Our image represented as a NumPy array has a shape of (height, width, channels), or (228,350,3).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Matrix definitions are typically defines in the form of (# of rows) x (# of columns).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We can also observe the image that we loaded has now appeared on the screen&lt;/p&gt;

&lt;figure&gt;
  
&lt;img src=&quot;https://handmap.github.io/images/posts/2016-08-06/OpenCV-image-read-save.png&quot; alt=&quot;Trex Output&quot; /&gt;

  &lt;figcaption&gt;OpenCV load_display_save.py output image&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Pressing any key removes the image from the screen and then saves a new copy of the image as ‘new_image.jpg’ in the directory where I ran the script.&lt;/p&gt;

&lt;h2 id=&quot;image-basics&quot;&gt;Image Basics&lt;/h2&gt;
&lt;hr /&gt;

&lt;h3 id=&quot;pixels&quot;&gt;pixels&lt;/h3&gt;
&lt;hr /&gt;

&lt;p&gt;Images consist of a set of pixels. These pixels are the fundamental building blocks of a image. A Pixel can be thought of a the “colour” or the “intensity” of light that appears at any given place in our image.&lt;/p&gt;

&lt;p&gt;looking back to my trex.png image file, It had a resolution of 350 pixels * 228 pixels. This means that the image has a whopping 79800 pixels all up. Each of these pixels can be represented in two ways:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Grayscale
    &lt;ul&gt;
      &lt;li&gt;Each pixel has a value between 0 and 255&lt;/li&gt;
      &lt;li&gt;This value represents each darker colour shift from white through to black.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Colour
    &lt;ul&gt;
      &lt;li&gt;Colour is made up of three RGB components (red, green and blue).&lt;/li&gt;
      &lt;li&gt;We store three sets of values between 0 and 255.&lt;/li&gt;
      &lt;li&gt;Each of these values is normally an 8-bit unsigned integer.&lt;/li&gt;
      &lt;li&gt;These three 8bit values are combined into an RGB tuple in the form of (red, green, blue).&lt;/li&gt;
      &lt;li&gt;Example: (255,255,255) is the colour White, while (255,0,0) is the colour red.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;image-coordinates&quot;&gt;image coordinates&lt;/h3&gt;
&lt;hr /&gt;

&lt;p&gt;Image coordinates can be best examples by looking at the reference image below taken from the Practical Python and OpenCV tutorial book.&lt;/p&gt;

&lt;figure&gt;
  
&lt;img src=&quot;https://handmap.github.io/images/posts/2016-08-06/Practical-Python-Coordinates.png&quot; alt=&quot;Image Coordinates&quot; /&gt;

  &lt;figcaption&gt;Image coordinate example from PPaOCV book&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Above is an 8x8 grid containing 64 individual pixels. You can see how easy it is for us to reference a particular point in an image grid by simply referencing the (row,column) coordinates of the position.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Note that Python and many other languages reference arrays of data starting from an index of 0. This means that the first position in the top left of our image is actually (0,0) and not (1,1)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;accessing-and-manipulating-pixels&quot;&gt;Accessing and Manipulating Pixels&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Next we’re going to try to manipulate our trex image in some way using what we just learnt about pixels and pixel coordinates. To start with I’m going to insert the following base code to setup our import image and display it with the title “Original”&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;__future__&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;print_function&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;argparse&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;cv2&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ap&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;argparse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ArgumentParser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_argument&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;-i&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;--image&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;required&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;help&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Path to the image&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;vars&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parse_args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;image&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Original&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Each pixel in the ‘image’ dictionary that we produce with the code above can be referenced by its coordinates and contains a tuple representing its RGB colour.&lt;/p&gt;

&lt;p&gt;I used the following code to check that the data was in fact in tuple form.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# place the prettyprint import at the top of the file&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pprint&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;x value: &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
              &lt;span class=&quot;s&quot;&gt;&quot; y value: &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
              &lt;span class=&quot;s&quot;&gt;&quot; content: &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pprint&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pformat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This code presents all 79800 pixel entries with their corresponding RGB values. You can see a small example of this below.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;x value: 148 y value: 107 content: array&lt;span class=&quot;o&quot;&gt;([&lt;/span&gt;87, 92, 87], &lt;span class=&quot;nv&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;uint8&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
x value: 148 y value: 108 content: array&lt;span class=&quot;o&quot;&gt;([&lt;/span&gt;81, 87, 81], &lt;span class=&quot;nv&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;uint8&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
x value: 148 y value: 109 content: array&lt;span class=&quot;o&quot;&gt;([&lt;/span&gt;67, 74, 68], &lt;span class=&quot;nv&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;uint8&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
x value: 148 y value: 110 content: array&lt;span class=&quot;o&quot;&gt;([&lt;/span&gt;81, 89, 83], &lt;span class=&quot;nv&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;uint8&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
x value: 148 y value: 111 content: array&lt;span class=&quot;o&quot;&gt;([&lt;/span&gt;108, 118, 111], &lt;span class=&quot;nv&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;uint8&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
x value: 148 y value: 112 content: array&lt;span class=&quot;o&quot;&gt;([&lt;/span&gt; 92, 101,  93], &lt;span class=&quot;nv&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;uint8&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
x value: 148 y value: 113 content: array&lt;span class=&quot;o&quot;&gt;([&lt;/span&gt;76, 86, 77], &lt;span class=&quot;nv&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;uint8&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
x value: 148 y value: 114 content: array&lt;span class=&quot;o&quot;&gt;([&lt;/span&gt;100, 110, 101], &lt;span class=&quot;nv&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;uint8&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
x value: 148 y value: 115 content: array&lt;span class=&quot;o&quot;&gt;([&lt;/span&gt; 97, 108,  99], &lt;span class=&quot;nv&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;uint8&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
x value: 148 y value: 116 content: array&lt;span class=&quot;o&quot;&gt;([&lt;/span&gt;111, 125, 116], &lt;span class=&quot;nv&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;uint8&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
x value: 148 y value: 117 content: array&lt;span class=&quot;o&quot;&gt;([&lt;/span&gt;84, 96, 89], &lt;span class=&quot;nv&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;uint8&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
x value: 148 y value: 118 content: array&lt;span class=&quot;o&quot;&gt;([&lt;/span&gt;79, 86, 79], &lt;span class=&quot;nv&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;uint8&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
x value: 148 y value: 119 content: array&lt;span class=&quot;o&quot;&gt;([&lt;/span&gt;56, 58, 52], &lt;span class=&quot;nv&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;uint8&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Something very important to note about the structure of the RGB tuple is that OpenCV stores the RGB channels in reverse order. So the normal [RED, GREEN, BLUE] is actually [BLUE, GREEN, RED]&lt;/p&gt;

&lt;p&gt;I’ll demonstrate this with the following code that prints each pixel in the image with the colour label.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]):&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;blue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;green&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;red&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Pixel at (&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;,&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;) - &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
              &lt;span class=&quot;s&quot;&gt;&quot;Red: {}, Green: {}, Blue: {}&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;red&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;green&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;blue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Output&lt;/span&gt;
Pixel at &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;270,201&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; - Red: 239, Green: 240, Blue: 244
Pixel at &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;270,202&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; - Red: 239, Green: 240, Blue: 244
Pixel at &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;270,203&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; - Red: 239, Green: 240, Blue: 244
Pixel at &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;270,204&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; - Red: 238, Green: 239, Blue: 243
Pixel at &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;270,205&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; - Red: 238, Green: 239, Blue: 243
Pixel at &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;270,206&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; - Red: 238, Green: 239, Blue: 243
Pixel at &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;270,207&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; - Red: 238, Green: 239, Blue: 243
Pixel at &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;270,208&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; - Red: 238, Green: 239, Blue: 243
Pixel at &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;270,209&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; - Red: 237, Green: 238, Blue: 242
Pixel at &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;270,210&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; - Red: 237, Green: 238, Blue: 242
Pixel at &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;270,211&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; - Red: 237, Green: 238, Blue: 243
Pixel at &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;270,212&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; - Red: 236, Green: 237, Blue: 242
Pixel at &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;270,213&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; - Red: 236, Green: 237, Blue: 242
Pixel at &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;270,214&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; - Red: 236, Green: 237, Blue: 242
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;accessing-portions-of-an-image&quot;&gt;Accessing portions of an Image&lt;/h3&gt;
&lt;hr /&gt;

&lt;p&gt;You can also access specific parts of an image instead of the whole thing. This kind of technique is important as you don’t want to be having to interate over the same image in its entirity every single time you’re running checks.&lt;/p&gt;

&lt;p&gt;NumPy provides a technique called array slicing for this very problem. Lets take a look at the following code.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;__future__&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;print_function&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;argparse&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;cv2&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ap&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;argparse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ArgumentParser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_argument&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;-i&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;--image&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;required&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;help&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Path to the image&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;vars&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parse_args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;image&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Original&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# grab 100 x 100 pixel region of the image&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;corner&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Corner&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;corner&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# change the colour of this region to green&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# present the updated image&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Updated&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;waitKey&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The four expected indexes that we are expected to provide when slicing are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Start y
    &lt;ul&gt;
      &lt;li&gt;our slice starts at y = 0.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;End y
    &lt;ul&gt;
      &lt;li&gt;our slice stops alone the y-axis when y = 100.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Start x
    &lt;ul&gt;
      &lt;li&gt;our top left x slice starts at x = 0.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;End x
    &lt;ul&gt;
      &lt;li&gt;our slice stops when the x-axis is x = 100.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;All seemed to go well and I was presented with the output shown below. Note that I generated a number of different window sessions for each step I performed.&lt;/p&gt;

&lt;figure&gt;
  
&lt;img src=&quot;https://handmap.github.io/images/posts/2016-08-06/OpenCV-crop-image.png&quot; alt=&quot;Image Crop&quot; /&gt;

  &lt;figcaption&gt;OpenCV Image crop&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Today was very productive and ended up getting through a good portion of the tutorials. I am not comfortable doing the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Installing OpenCV requirements on Ubuntu.&lt;/li&gt;
  &lt;li&gt;Loading and Saving images with OpenCV in python.&lt;/li&gt;
  &lt;li&gt;Manipulating Pixels and image content with Splicing and For Loops.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In my next session I will be working on drawing boxes and lines over the top of images and hopefully also working with live video from my computers webcam.&lt;/p&gt;</content><category term="python" /><category term="opencv" /><summary>I delve deeper into OpenCV using the Python language and explore how it works at a lower level</summary></entry><entry><title>Project Introduction</title><link href="https://handmap.github.io/project-introduction/" rel="alternate" type="text/html" title="Project Introduction" /><published>2016-08-05T00:00:00+08:00</published><updated>2016-08-05T00:00:00+08:00</updated><id>https://handmap.github.io/project-introduction</id><content type="html" xml:base="https://handmap.github.io/project-introduction/">&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Hand Map is the title I’ve given the project I’m currently in the process of implementing for my final year Computer Systems and Networking project with Curtin University.&lt;/p&gt;

&lt;p&gt;The goal is to create an accessible way for people living with cerebral palsy to generate their own splint sketch files that could be 3D printed; saving them regular trips to specialised medical centers.&lt;/p&gt;

&lt;h2 id=&quot;requirements&quot;&gt;Requirements&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;The requirements for this project will be fairly dynamic, as the expected deliverable’s will change depending on how smoothly the early stages of the project go.&lt;/p&gt;

&lt;p&gt;Current requirement are as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Hand Detection&lt;/li&gt;
  &lt;li&gt;Object Detection&lt;/li&gt;
  &lt;li&gt;Hand Mapping based on Object’s position on Hand&lt;/li&gt;
  &lt;li&gt;3D Restructure of Hand model from 2D Images&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;special-thanks&quot;&gt;Special Thanks&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Halve Jekyll Theme: &lt;a href=&quot;https://github.com/TaylanTatli/Halve&quot;&gt;https://github.com/TaylanTatli/Halve&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Jekyll: &lt;a href=&quot;https://jekyllrb.com/&quot;&gt;https://jekyllrb.com/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Github Pages: &lt;a href=&quot;https://pages.github.com/&quot;&gt;https://pages.github.com/&lt;/a&gt;&lt;/p&gt;</content><category term="project" /><summary>An introduction and breakdown of what is required for this project to be successful</summary></entry><entry><title>Getting started with OpenCV</title><link href="https://handmap.github.io/getting-started-with-opencv/" rel="alternate" type="text/html" title="Getting started with OpenCV" /><published>2016-08-05T00:00:00+08:00</published><updated>2016-08-05T00:00:00+08:00</updated><id>https://handmap.github.io/getting-started-with-opencv</id><content type="html" xml:base="https://handmap.github.io/getting-started-with-opencv/">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;I’ve decided to investigate some of the computer vision libaries that are already available that could possibly already do what I need. This is likely to save me a lot of time not having to re-invent the wheel.&lt;/p&gt;

&lt;p&gt;I had the idea to look into OpenCV when I noticed a particular repository trending on &lt;a href=&quot;https://github.com/&quot;&gt;Github&lt;/a&gt; the other day. The repo in question; maintained by &lt;a href=&quot;https://github.com/Shinao&quot;&gt;Raphael Monnerat&lt;/a&gt; was titled &lt;a href=&quot;https://github.com/Shinao/SmartMirror&quot;&gt;SmartMirror&lt;/a&gt; and is designed to turn a two-way mirror into a gesture controlled smart device.&lt;/p&gt;

&lt;figure&gt;
  
&lt;img src=&quot;https://handmap.github.io/images/posts/2016-08-05/SmartMirror_DisplayMenu_Preview.gif&quot; alt=&quot;Smart Mirror&quot; /&gt;

  &lt;figcaption&gt;Raphael using Smart Mirror&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The potential for me to apply a similar algorithm to the one he was using to my project was almost too obvious, as it looked as though his implementation used some form of Hand Detection/Recognition. He even had a debugging tool packaged with his code that would display the gesture recognition information in real time while testing.&lt;/p&gt;

&lt;figure&gt;
  
&lt;img src=&quot;https://handmap.github.io/images/posts/2016-08-05/SmartMirror_Debug.png&quot; alt=&quot;Smart Mirror Debug&quot; /&gt;

  &lt;figcaption&gt;Smart Mirror Debug&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;installing-opencv&quot;&gt;Installing OpenCV&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;My first task was to install OpenCV on a system that I could use to test the SmartMirror code. I span up an Ubuntu 16.04.1 instance and ran the following code to setup the latest OpenCV version on the system.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;## Install pre-requisites&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;sudo apt-get update
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;sudo apt-get upgrade
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;sudo apt-get install build-essential
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;sudo apt-get install cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;sudo apt-get install python-dev python-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev

&lt;span class=&quot;c&quot;&gt;## Change directory into a working folder&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; ~/Documents/

&lt;span class=&quot;c&quot;&gt;## Clone OpenCV repo and build from source&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;git clone https://github.com/opencv/opencv.git
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;opencv/
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;mkdir release
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;release/
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;cmake -D &lt;span class=&quot;nv&quot;&gt;CMAKE_BUILD_TYPE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;RELEASE -D &lt;span class=&quot;nv&quot;&gt;CMAKE_INSTALL_PREFIX&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/usr/local ..
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;make
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;sudo make install
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Now that I had the latest version of OpenCV installed I needed to make sure it was working. I also decided that my language of choice; particularly during the initial testing phases would be Python.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;## Change into the OpenCV samples directory&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;~/Documents&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;opencv/samples/python/

&lt;span class=&quot;c&quot;&gt;## Either run each tool separately or use demo.py to list all demos&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;python demo.py 
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;I had a play around with a number of the built in tools and was very impressed by the level of quality and detailed code each example provided. One of my personal favorites was the ‘edge.py’ example as I felt like it would provide a very good starting point for when I begin writing my hand detection method.&lt;/p&gt;

&lt;figure&gt;
  
&lt;img src=&quot;https://handmap.github.io/images/posts/2016-08-05/OpenCV-Edgepy-test.png&quot; alt=&quot;OpenCV Edge Detection&quot; /&gt;

  &lt;figcaption&gt;OpenCV Canny Edge Detection&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;smartmirror-analysis&quot;&gt;SmartMirror Analysis&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Now that I had OpenCV installed I decided I would attempt to demo the SmartMirror hand detection code to see if it was as good as it looked like it was.&lt;/p&gt;

&lt;p&gt;I started out by cloning the repository using git and locating the ‘test.py’ file mentioned in the repo README.md.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;## Change into SmartMirror/Motion directory&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;git clone https://github.com/Shinao/SmartMirror.git
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;SmartMirror/Motion

&lt;span class=&quot;c&quot;&gt;## Execute the test.py script&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;python test.py
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The example was very finicky to get working, The hand detection itself works very well when it isn’t getting confused about the background lighting.&lt;/p&gt;

&lt;figure&gt;
  
&lt;img src=&quot;https://handmap.github.io/images/posts/2016-08-05/SmartMirror_Debug_Nathan_3slide.png&quot; alt=&quot;SmartMirror Hand Detection&quot; /&gt;

  &lt;figcaption&gt;SmartMirror Hand Detection&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Difficulty assign, the implementing was detecting my palm, and there was even logic in place to handle hand movements (swiping in different directions).&lt;/p&gt;

&lt;figure&gt;
  
&lt;img src=&quot;https://handmap.github.io/images/posts/2016-08-05/SmartMirror_Debug_Nathan_gesture.png&quot; alt=&quot;SmartMirror Hand Detection&quot; /&gt;

  &lt;figcaption&gt;SmartMirror Gesture&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;Today was a productive session that really got me thinking about the possibilities that OpenCV has to offer. Now that I know that what I want to achieve is very possible with the OpenCV libraries I believe my next step will be to learn the OpenCV frameworks from scratch. My goal for the next week is to build up a small library myself so that I can begin to understand how other peoples code works without having to guess/hack a solution together.&lt;/p&gt;

&lt;p&gt;I picked up a digital copy of &lt;a href=&quot;https://www.pyimagesearch.com/practical-python-opencv/&quot;&gt;Practical Python and OpenCV&lt;/a&gt; by Adrian Rosebrock as I’ve had it recommended to me before as a great practical reasource for learning the in’s and out’s of OpenCV.&lt;/p&gt;

&lt;p&gt;Until next time,&lt;/p&gt;

&lt;p&gt;Nathan.&lt;/p&gt;</content><category term="project" /><category term="opencv" /><category term="ubuntu" /><summary>I begin to investigate one of the most popular open source computer vision libraies, OpenCV</summary></entry></feed>
